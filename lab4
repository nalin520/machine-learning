from numpy.core.numeric import False_
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix,accuracy_score
from numpy.linalg import det, inv

data = pd.read_csv("SteelPlateFaults-2class.csv")

#Q1. splitting data and applying knn
X1 = data[data['Class']==0].iloc[:,:-1]
X2 = data[data['Class']==1].iloc[:,:-1]
X_label1 = data[data['Class']==0]['Class']
X_label2 = data[data['Class']==1]['Class']
[X_train1, X_test1, X_label_train1, X_label_test1] = train_test_split(X1, X_label1, test_size=0.3, random_state=42, shuffle=True)
[X_train2, X_test2, X_label_train2, X_label_test2] = train_test_split(X2, X_label2, test_size=0.3, random_state=42, shuffle=True)

train_data1=X_train1.merge(X_label_train1,left_index=True,right_index=True) #Merging train labels with train data to get full training data of class 0
test_data1=X_test1.merge(X_label_test1,left_index=True,right_index=True)  #Merging test labels with test data to get full testing data of class 0
train_data2=X_train2.merge(X_label_train2,left_index=True,right_index=True) #Merging train labels with train data to get full training data of class 1
test_data2=X_test2.merge(X_label_test2,left_index=True,right_index=True)  #Merging test labels with test data to get full testing data of class 1

train_data = pd.concat([train_data1,train_data2]) #concatinating 
test_data = pd.concat([test_data1,test_data2])

train_data.to_csv('SteelPlateFaults-train.csv',index=False)
test_data.to_csv('SteelPlateFaults-test.csv',index=False)

k_values=[1,3,5] #All values of k for K-NN
for k in k_values:
    knn=KNeighborsClassifier(n_neighbors=k) #Creatiing KNN object 
    knn.fit(train_data.iloc[:,:-1],train_data.iloc[:,-1])  #Fitting the train data
    predictions=knn.predict(test_data.iloc[:,:-1])     #getting the predictions

    conf_mat=confusion_matrix(test_data.iloc[:,-1],predictions,labels=[0,1]) #Computing Conf. Mtrix
    print("for k=%i (acc: %.3f)"%(k,100*accuracy_score(test_data.iloc[:,-1],predictions)))  #Print accuracy
    print(conf_mat,"\n")

#Q2. normalizing and predicting through knn
label_train = train_data['Class']

train_data.drop('Class',axis=1)
test_data.drop('Class',axis=1)

normalized_train=(train_data-train_data.min())/(train_data.max()-train_data.min())    #Normalizing the train_data
normalized_test=(test_data-train_data.min())/(train_data.max()-train_data.min())  #Normalizing the test data
normalized_train.to_csv('SteelPlateFaults-train-Normalised.csv',index=False)
normalized_test.to_csv('SteelPlateFaults-test-normalised.csv',index=False)

for k in k_values:
    knn=KNeighborsClassifier(n_neighbors=k)
    knn.fit(normalized_train.iloc[:,:-1],label_train)
    predictions=knn.predict(normalized_test.iloc[:,:-1])
    conf_mat=confusion_matrix(normalized_test.iloc[:,-1],predictions,labels=[0,1])
    print("for k=%i:(acc:%.3f)"%(k,100*accuracy_score(normalized_test.iloc[:,-1],predictions)))
    print(conf_mat,"\n")


from numpy.core.numeric import False_
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix,accuracy_score
from numpy.linalg import det, inv

train_data = pd.read_csv("SteelPlateFaults-train.csv")
test_data = pd.read_csv("SteelPlateFaults-test.csv")

#Q3. building and training bayes model

# defining the likelihood function
def prob(x, inv_cov_mat, mu_mat, Pci):
    diff = x- mu_mat
    diff_T = diff.T
    dotprod = np.dot(diff.T, inv_cov_mat)
    prod = np.dot(dotprod, diff)
    return np.log(Pci)+0.5*np.log(det(inv_cov_mat))-11.5*np.log(2*np.pi)-0.5*prod

# remove the following attributes since they make the covariance matrix singular
delete_items = ["TypeOfSteel_A300", "TypeOfSteel_A400", "X_Minimum", "Y_Minimum"]
for items in delete_items:
    train_data.pop(items)
    test_data.pop(items)

# separate the classes from the training dataset
df_class1 = train_data[train_data["Class"] == 1].copy()
df_class1.__delitem__("Class")
df_class0 = train_data[train_data["Class"] == 0].copy()
df_class0.__delitem__("Class")

x_train = train_data.iloc[:, :-1].values
y_train = train_data.iloc[:, train_data.shape[1] - 1].values
x_test = test_data.iloc[:, :-1].values
y_test = test_data.iloc[:, test_data.shape[1] - 1].values

# compute mean vectors from both classes

mu1 = df_class1.mean()
mu0 = df_class0.mean()
inv1 = inv(df_class1.cov())
inv0 = inv(df_class0.cov())
pd.DataFrame(mu0).to_csv("mean0.csv")
pd.DataFrame(mu1).to_csv("mean1.csv")
df_class1.cov().to_csv("cov1.csv")
df_class0.cov().to_csv("cov0.csv")

# predict based on bayes classifier

pred = []
for x in test_data.index:
    p1 = prob(test_data.iloc[x, :-1].to_numpy(), inv1, mu1, 509/(509+273))
    p0 = prob(test_data.iloc[x, :-1].to_numpy(), inv0, mu0, 273/(509+273))
    if p1 > p0:
        pred.append(1)
    else:
        pred.append(0)

print("The confusion matrix for Bayes model is\n", confusion_matrix(y_test, pred))
print("\n")
print("The accuracy for Bayes model is", accuracy_score(y_test, pred))
print("\n")


#Q4
print("Best Acc:")
print("\t For KNN (k=3)",89.614)
print("\t For KNN(k=5) with normalization :", 97.329)
print("\t For Bayes: ",94.362)
